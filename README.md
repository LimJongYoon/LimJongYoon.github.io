<!DOCTYPE html>

<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Portfolio</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>

## Publications <!-- {docsify-ignore} -->

<div class="portfolio-item" onclick="window.location.href='#/GuideDogAR.md';">
    <img src="../images/blind.png" alt="Guide Dog AR Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">Guide Dog AR: A Tactile and Auditory Assisting Device Design with the Motif of a Guide Dog for the Visually Impaired</span>
        <span class="portfolio-middle">Soo Min Shin, <strong>Jongyoon Lim</strong>, Yongsoon Choi<br>
        International Journal of Human–Computer Interaction, 1-14</span><br>
        <span class="summary">This study introduces “Guide Dog AR,” an AR device inspired by the handle of a guide dog’s harness. The device and its associated content aim to provide visually impaired individuals with the experience of walking with a guide dog, serving as both a practical tool and a foundation for entertainment. The device offers an augmented walking experience through tactile and auditory sensations. The device was assessed with the factors "effectiveness" and "immersiveness." The evaluation for effectiveness involved creating a simulated virtual path. The level of immersion was evaluated based on how deeply users were engrossed in the augmented content provided by the device. The success rate was determined by how well users navigated this virtual path using the device’s feedback. In assessing immersion, in-depth interviews were conducted with visually impaired participants who experienced the virtual path...</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/HapTug.md';">
    <img src="../images/haptug.png" alt="HapTug Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">HapTug: Force-Feedback Haptic Device for Representation of Tugs in Virtual Reality</span>
        <span class="portfolio-middle"><strong>Jongyoon Lim</strong>, Yongsoon Choi<br>
        Electronics 11 (11), 1730</span><br>
        <span class="summary">In this study, a haptic device is proposed for virtual reality (VR). The proposed haptic device is linked to the bundle controller of a VR device. The device proposed in this study was a force-feedback device that focused on the haptic felt by the hand rather than the feeling of the tool colliding with the object. The device provides the feeling of centrifugal or inertial forces due to the weight of the tool when the user swings the tool, and the feeling of reaction force when a user stabs or pokes an object with the tool. The force generated by a haptic device of the linked type was measured and examined to determine whether this feeling of actually using a tool in the virtual environment is provided. In addition, we conducted two types of experiments to evaluate this haptic. First, VR content was created for user experience, and a survey was conducted with 30 experimenters to compare the bundle controllers and haptic device, called HapTug, in terms of realism, immersion, and enjoyment. The survey results demonstrated that the realism, immersion, and enjoyment increased compared to those using the bundle controller alone. Second, 41 experimenters were allowed to play freely in a virtual environment for six sessions, 30 seconds each. They had to press the button to control 30 seconds and the device they wore turned on and off randomly every session. During the experience, their playback time, number of interactions, and range were measured. As a result, when the device activated, experimenters felt the time was shorter and showed widespread movements. Thus, it was proven that the proposed linked-type haptic device effectively delivered the feeling of wielding a tool, allowing the user to feel the virtual object coming in and going out of the hand when interacting with it. Moreover, three possibilities were found in the experiments. Previous VR experience times have little to do with the satisfaction of haptic use, users have low expectations for haptic realism, and users interact more actively with haptic devices.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/BirdVR.md';">
    <img src="../images/birdVR.png" alt="BirdVR Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">BirdVR: UX design that enhances the connection between the form of the VR controller and the interaction within the content</span>
        <span class="portfolio-middle">Kyoungmin Bang, <strong>Jongyoon Lim</strong>, Haree Jun, Hangyeol Jo, Jean Ho Chu<br>
        HCIK(HCI Korea)(2022): 813-815</span><br>
        <span class="summary">The BirdVR project is a UI/UX design initiative aimed at developing a bird-shaped VR controller tailored specifically for seniors aged 65 and older. This project addresses two key challenges: the difficulty seniors experience with traditional VR controllers and the disruption of immersion caused by complex interactive functions. The BirdVR controller offers an innovative solution by adopting a bird-like form, which serves dual purposes: it simplifies the learning process by aligning with the VR environment and enhances psychological stability through the soothing experience of petting the bird-shaped device. The controller is equipped with interactive touch areas, made possible by conductive thread, allowing for intuitive and simplified interaction within VR environments. Based on user feedback, motorized wings initially integrated into the design were removed to preserve the simplicity and comfort of the experience. The BirdVR controller is currently implemented at a welfare center in Gogang-dong, Seoul, where it is utilized in psychological healing VR content for seniors. The project has been recognized at HCI Korea 2022, where it was featured in the Interactive Art Gallery as part of the Creative Award showcase.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

## Project <!-- {docsify-ignore} -->

#### Tangible Products <!-- {docsify-ignore} -->

<div class="portfolio-item" onclick="window.location.href='#/everpen2.md';">
    <img src="../images/everpen2.png" alt="everpen2" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">EverPen 2.0</span>
        <span class="portfolio-middle">Exhibited at CES 2024, 2025, Korean Government Minister’s Award<br>
        Concept Design, Circuit Design, Hardware/Software Development, Sales</span><br>
        <span class="summary">EverPen 2.0 is poised to revolutionize the spatial computing landscape, just as the mouse transformed desktop computing and the Apple Pencil reshaped the tablet market. Moving beyond the limitations of traditional VR controllers, EverPen 2.0 operates independently, accurately tracking its position in a 3D coordinate system without the need for external stations. By combining advanced hand tracking with integrated touch and physical sensors, EverPen 2.0 delivers unmatched precision and freedom in virtual environments. As the next-generation HID, EverPen 2.0 is set to redefine how we interact with virtual reality, making spatial computing more intuitive, efficient, and immersive than ever before..</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/everpen1.md';">
    <img src="../images/everpen1.png" alt="everpen1" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">EverPen 1.0</span>
        <span class="portfolio-middle">Government Investment Secured, Exhibited at Korea Electronics Show 2023<br>
        Circuit Design, Hardware/Software Development</span><br>
        <span class="summary">EverPen 1.0 is a VR accessory designed to enhance the input experience in virtual reality by serving as a pen-like input device. By combining with a VR controller, it offers a more natural and intuitive way to interact with virtual environments compared to the traditional VR controller's shooting button. While VR pens have been developed previously to address this issue, they often have limitations, such as being limited to specific spaces or lacking compatibility with existing VR controllers. In contrast, the EverPen 1.0 can seamlessly integrate with a VR controller, allowing users to write or draw in virtual reality while wirelessly transmitting and displaying the information on the head-mounted display (HMD). This innovative solution eliminates the need for a separate positioning camera system, which was previously required in limited-space setups. With the EVER Pen, users can enjoy a more immersive and fluid VR interaction experience while utilizing familiar pen-like input gestures.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/jp/Idealavor.md';">
    <img src="../images/Idealavor.png" alt="Idealavor Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">Idealavor: Multi-Modality Eating Texture Haptic Device</span>
        <span class="portfolio-middle">Electronics and Telecommunications Research Institute<br>
        Concept Design & Industrial Design</span><br>
        <span class="summary">The Idealavor project explores the recreation of food textures in the food industry, focusing on how eating texture influences satiety and taste perception. Traditional attempts to replicate the texture of high-quality ingredients with cheaper alternatives have often failed due to differences in price and texture. This project addresses these challenges by leveraging multi-modality approaches—combining visual, auditory, and haptic feedback to enhance the perception of food texture. The device includes texture learning tailored to individual chewing patterns, visual augmentation using color, auditory enhancement of chewing sounds, and EMS (Electrical Muscle Stimulation) to simulate the feeling of chewing directly on the jaw muscles.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/cradle.md';">
    <img src="../images/cradle.png" alt="cradle" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">Smart Cradle</span>
        <span class="portfolio-middle">AI-driven Health Monitoring for Newborns<br>
        Concept Design, Industrial Design, Circuit Design, Hardware/Software Development, Sales</span><br>
        <span class="summary">The Smart Cradle is designed to prevent sudden infant death. While wearable healthcare devices are widely available for adults, there is a significant lack of such devices for newborns, despite the critical importance of monitoring their health. The Smart Cradle and Smart Patch bridge this gap by leveraging advanced AI-driven algorithms to monitor key health indicators such as facial expressions, weight, air quality, oxygen saturation (SpO2), oxygen levels, and pulse. This comprehensive monitoring system enables the prediction of dietary needs, bowel movements, and overall health status, making it an essential tool for safeguarding the well-being of newborns.</span> 
        <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/jp/table-it.md';">
    <img src="../images/tableit.png" alt="Table-It Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">Table-It: Motion and Color Recognition AR Table</span>
        <span class="portfolio-middle">Korean Government Minister’s Award, Efesta2017 (KOREA)<br>
        Concept Design, Software Development</span><br>
        <span class="summary">The smart table using vision recognition and a projector won the Minister Award at 2017 EFESTA. A smart table was created where people can enjoy various contents like a giant tablet. Using Python, a mouse cursor is implemented by hand motion recognition, and clicking is possible by folding and unfolding the hand. Thus, users can operate all programs typically used on a computer. Additionally, through color recognition, dedicated applications like table tennis and billiards, which cannot be enjoyed on a regular tablet, were developed.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

#### Arts and Games <!-- {docsify-ignore} -->

<div class="portfolio-item" onclick="window.location.href='#/jp/MMCA.md';">
    <img src="../images/MMCA.png" alt="MMCA Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">AI Docent</span>
        <span class="portfolio-middle">IOS App, National Museum of Modern and Contemporary Art (MMCA)<br>
        Concept Design, UX/UI Design & Software Development</span><br>
        <span class="summary">An innovative AR platform that transforms multimedia contemporary art into an immersive, interactive experience. Utilizing BLE interior positioning, it creates an engaging environment where each artwork becomes a unique interactive experience.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/jp/BearHands.md';">
    <img src="../images/bearhands.png" alt="BearHands Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">BearHands</span>
        <span class="portfolio-middle">Game</span>
        <span class="portfolio-middle">3D Modeling, UX/UI Design & Software Development</span><br>
        <span class="summary">BearHands is a wearable device that enhances user interaction in AR/VR environments by providing haptic feedback and gesture recognition, making virtual experiences feel more natural and immersive.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/jp/Stern.md';">
    <img src="../images/Stern.png" alt="Stern Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">Stern: Interactive Lighting Art</span>
        <span class="portfolio-middle">Exhibition</span>
        <span class="portfolio-middle">Concept Design & Hardware Development</span><br>
        <span class="summary">Stern is an interactive lighting installation that responds to human presence and movement, creating a dynamic art piece that evolves with its environment, integrating light and motion into an artistic experience.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/jp/Stern.md';">
    <img src="../images/space2.png" alt="Starbound Odyssey Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">Starbound Odyssey</span>
        <span class="portfolio-middle">VR Game</span>
      <span class="portfolio-middle">Concept Design, Texturing, Shading & Software Development</span><br>
        <span class="summary">"Starbound Odyssey" is a VR game that immerses players in the experience of a spacewalk, with a focus on toon shading and texturing. The game features a vibrant, cartoon-like aesthetic that contrasts with the realistic physics of space, creating a unique visual style. Players explore beautifully textured cosmic environments, drifting among stars and interacting with celestial objects, all within a richly detailed and stylized universe.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/jp/HotelMeta.md';">
    <img src="../images/hotelmeta.png" alt="Hotel Meta Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">Hotel Meta</span>
        <span class="portfolio-middle">Exhibition</span>
        <span class="portfolio-middle">Hardware Development</span><br>
        <span class="summary">This exhibition operates as a digital twin, functioning simultaneously in both the real and virtual worlds through VR. It mirrors the physical space in real-time, and visitors who purchase an NFT link can access additional interactions not included in the main exhibition. The audience experiences the artist’s unconscious through the metaverse, with the hotel symbolizing a space of empathy between the artist’s inner world and the audience. This exhibition provides a unified experience that merges virtual and real environments, exploring new ways for art to engage and communicate with the audience.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/jp/BLE.md';">
    <img src="../images/BLE2.png" alt="BLE Interior Positioning Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">BLE Interior Positioning Base, AR Platform That Becomes a Work of Art</span>
        <span class="portfolio-middle">Exhibition, National Museum of Korea Project (2020)<br>
        Concept Design, UX/UI Design & Software Development</span><br>
        <span class="summary">This project leverages BLE (Bluetooth Low Energy) technology to create an innovative AR platform within museums and exhibition halls. By implementing precise interior positioning, the platform enhances visitor experience by dynamically generating AR content based on visitor movement and interaction data. The AR installations adapt to the frequency of footsteps, creating a living, evolving art piece that grows and changes with each visitor's path. The system also integrates social media interaction by displaying Instagram photos related to the exhibition, creating a unique digital guestbook that enriches the physical space with user-generated content.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

<div class="portfolio-item" onclick="window.location.href='#/jp/vrfilms.md';">
    <img src="../images/vrfilm.png" alt="VR Films Image" class="portfolio-image">
    <div class="portfolio-text">
        <span class="portfolio-title">Eliminate the Sense of Heterogeneity of VR Devices by Wearing Goggles in a Virtual Environment</span>
        <span class="portfolio-middle">VR Movie, KT Super VR Contest (2020)<br>
        Concept Design & Software Development</span><br>
        <span class="summary">The Glasses is an immersive VR film where the audience interacts with the story in the virtual environment through physical gestures performed in the real world. The film revolves around the relationship between two lovers, with users switching between past and present scenes by wearing and removing virtual sunglasses. This interaction allows the audience to actively participate in the narrative, rather than just passively watching the film. By simulating real-world actions within VR, The Glasses enhances emotional immersion and adds depth to the story. This unique experience makes the storytelling of The Glasses even more engaging.</span> <a href="javascript:void(0);" onclick="toggleText(this)">More</a>
    </div>
</div>

## CV <!-- {docsify-ignore} -->

## others <!-- {docsify-ignore} -->

</body>
</html>
